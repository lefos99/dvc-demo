stages:
  split_data:
    cmd: >-
      python code/split_data.py 
      --input data/raw/data.csv 
      --output data/processed
    deps:
    - data/raw/data.csv
    - code/split_data.py
    params:
    - split_data.test_size
    - split_data.random_state
    outs:
    - data/processed/train.csv
    - data/processed/test.csv

  train_model:
    cmd: >-
      python code/train_model.py 
      --train data/processed/train.csv 
      --output data/model
    deps:
    - data/processed/train.csv
    - code/train_model.py
    params:
    - train_model.n_estimators
    - train_model.max_depth
    - train_model.min_samples_split
    - train_model.min_samples_leaf
    - train_model.max_features
    - train_model.bootstrap
    - train_model.random_state
    - train_model.pos_label
    - train_model.use_feature_scaling
    outs:
    - data/model/random_forest_model.joblib
    - data/model/scaler.joblib
    - data/model/feature_importance.csv
    - data/model/training_info.json

  evaluate_model:
    cmd: >-
      python code/evaluate_model.py 
      --model data/model/random_forest_model.joblib
      --scaler data/model/scaler.joblib
      --test data/processed/test.csv 
      --output data/model/evaluation
    deps:
    - data/model/random_forest_model.joblib
    - data/model/scaler.joblib
    - data/processed/test.csv
    - code/evaluate_model.py
    params:
    - evaluate_model.pos_label
    - evaluate_model.top_features_to_plot
    outs:
    - data/model/evaluation/confusion_matrix.png
    - data/model/evaluation/roc_curve.png
    - data/model/evaluation/feature_importance.png
    - data/model/evaluation/prediction_analysis.png
    - data/model/evaluation/feature_importance.csv
    metrics:
    - data/model/evaluation/metrics.json 